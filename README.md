# DataOps Automation with Bash

![Bash](https://img.shields.io/badge/Bash-4EAA25?style=for-the-badge&logo=gnu-bash&logoColor=white)
![Shell Script](https://img.shields.io/badge/Shell_Script-121011?style=for-the-badge&logo=gnu-bash&logoColor=white)
![Datamash](https://img.shields.io/badge/Tool-Datamash-blue?style=for-the-badge&logo=gnu&logoColor=white)
![AWK](https://img.shields.io/badge/AWK-Processing-green?style=for-the-badge)
![Monitoring](https://img.shields.io/badge/Monitoring-Log%20Analysis-orange?style=for-the-badge)
![Mermaid](https://img.shields.io/badge/Diagrams-Mermaid-orange?style=for-the-badge&logo=mermaid&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-green.svg?style=for-the-badge)

---

## üáßüá∑ Automa√ß√£o DataOps com Bash

Este reposit√≥rio √© dedicado a demonstrar **solu√ß√µes pr√°ticas e eficientes para automa√ß√£o de tarefas DataOps utilizando scripts Bash**. O foco √© em como orquestrar, monitorar e gerenciar pipelines de dados, tarefas de ETL (Extract, Transform, Load), e rotinas de manuten√ß√£o de sistemas de dados, aproveitando a robustez e a flexibilidade do ambiente de linha de comando Linux. √â ideal para **engenheiros de dados, administradores de sistemas e profissionais de DevOps** que buscam otimizar e automatizar seus fluxos de trabalho de dados.

### üéØ Objetivo

O principal objetivo deste projeto √© **fornecer um conjunto de scripts Bash bem documentados e exemplos de casos de uso** para automatizar opera√ß√µes comuns em DataOps. Ser√£o abordados t√≥picos como agendamento de tarefas (cron), manipula√ß√£o de arquivos e diret√≥rios, intera√ß√£o com bancos de dados (via CLI), monitoramento de processos e tratamento de erros, tudo com foco em **escalabilidade, confiabilidade e facilidade de manuten√ß√£o**, com √™nfase em **valida√ß√£o de dados robusta, transforma√ß√µes complexas e um sistema de notifica√ß√£o de status**.

### ‚ú® Highlights

- **Valida√ß√£o de Dados Robusta**: Implementa√ß√£o de fun√ß√µes de valida√ß√£o que verificam a integridade e o formato dos dados em diferentes est√°gios da pipeline, garantindo a qualidade dos dados antes do processamento.
- **An√°lise Avan√ßada de Logs**: M√≥dulo `log_analyzer.sh` que demonstra an√°lise profissional de logs com detec√ß√£o de anomalias, an√°lise de padr√µes temporais, estat√≠sticas de tempos de resposta e gera√ß√£o de relat√≥rios automatizados.
- **Transforma√ß√µes de Dados Complexas**: Exemplos de como realizar transforma√ß√µes de dados sofisticadas usando ferramentas de linha de comando como `awk`, `sed`, `grep` e `datamash`, permitindo agrega√ß√£o, enriquecimento e reestrutura√ß√£o de dados.
- **Sistema de Notifica√ß√£o de Status**: Integra√ß√£o de mecanismos de notifica√ß√£o (e.g., e-mail, logs detalhados) para alertar sobre o sucesso, falha ou anomalias na execu√ß√£o da pipeline, facilitando o monitoramento proativo.
- **Automa√ß√£o de Pipelines**: Scripts para orquestrar o fluxo de dados, desde a ingest√£o at√© o processamento e carregamento, com tratamento de erros e logging detalhado.
- **Gerenciamento de ETL**: Solu√ß√µes para automatizar tarefas de extra√ß√£o, transforma√ß√£o e carregamento de dados de diversas fontes.
- **Monitoramento e Alerta**: Scripts para monitorar a sa√∫de dos sistemas de dados, o status das tarefas e enviar alertas em caso de falhas.
- **C√≥digo Profissional**: Scripts bem estruturados, com coment√°rios claros, seguindo as melhores pr√°ticas de shell scripting para garantir legibilidade e manutenibilidade.
- **Documenta√ß√£o Completa**: Cada script √© acompanhado de documenta√ß√£o detalhada, explicando seu prop√≥sito, par√¢metros, l√≥gica e exemplos de uso.
- **Testes Inclu√≠dos**: Suite completa de testes automatizados para garantir qualidade e confiabilidade.
- **An√°lise Est√°tica**: Uso de shellcheck para garantir boas pr√°ticas e c√≥digo livre de erros comuns.

### üöÄ Benef√≠cios do DataOps com Bash em A√ß√£o

A automa√ß√£o DataOps com Bash oferece uma abordagem leve e poderosa para gerenciar pipelines de dados. Este projeto ilustra como esses benef√≠cios s√£o explorados:

1.  **Efici√™ncia e Velocidade**: Scripts Bash s√£o executados diretamente no sistema operacional, proporcionando alta performance para tarefas de manipula√ß√£o de arquivos e processamento de texto, como visto nas transforma√ß√µes com `awk` e `datamash`.

2.  **Flexibilidade e Controle**: A linha de comando oferece controle granular sobre cada etapa da pipeline, permitindo personaliza√ß√£o e adapta√ß√£o a requisitos espec√≠ficos, desde a valida√ß√£o de dados at√© a notifica√ß√£o de status.

3.  **Baixo Custo e Baixa Complexidade**: Utiliza ferramentas nativas do sistema operacional, eliminando a necessidade de infraestrutura complexa ou licen√ßas de software caras, tornando-o acess√≠vel para diversos ambientes.

4.  **Integra√ß√£o com Ferramentas Existentes**: Facilmente integr√°vel com outras ferramentas e sistemas via CLI, permitindo a constru√ß√£o de pipelines h√≠bridas e a intera√ß√£o com bancos de dados, APIs e servi√ßos de nuvem.

5.  **Robustez e Tratamento de Erros**: A inclus√£o de `set -o pipefail` e fun√ß√µes de `handle_error` demonstra como construir pipelines resilientes que podem lidar com falhas de forma controlada e notificar os respons√°veis.

6.  **Simplicidade e Manutenibilidade**: Scripts bem escritos e documentados s√£o f√°ceis de entender, depurar e manter, mesmo para equipes com conhecimento limitado em linguagens de programa√ß√£o mais complexas.

---

## üá¨üáß DataOps Automation with Bash

This repository is dedicated to demonstrating **practical and efficient solutions for DataOps task automation using Bash scripts**. The focus is on how to orchestrate, monitor, and manage data pipelines, ETL (Extract, Transform, Load) tasks, and data system maintenance routines, leveraging the robustness and flexibility of the Linux command-line environment. It is ideal for **data engineers, system administrators, and DevOps professionals** looking to optimize and automate their data workflows.

### üéØ Objective

The main objective of this project is to **provide a set of well-documented Bash scripts and use case examples** to automate common operations in DataOps. Topics covered include task scheduling (cron), file and directory manipulation, database interaction (via CLI), process monitoring, and error handling, all with a focus on **scalability, reliability, and ease of maintenance**, with an emphasis on **robust data validation, complex transformations, and a status notification system**.

### ‚ú® Highlights

- **Robust Data Validation**: Implementation of validation functions that check data integrity and format at different pipeline stages, ensuring data quality before processing.
- **Complex Data Transformations**: Examples of how to perform sophisticated data transformations using command-line tools like `awk`, `sed`, `grep`, and `datamash`, allowing for data aggregation, enrichment, and restructuring.
- **Status Notification System**: Integration of notification mechanisms (e.g., email, detailed logs) to alert about pipeline success, failure, or anomalies, facilitating proactive monitoring.
- **Pipeline Automation**: Scripts for orchestrating data flow, from ingestion to processing and loading, with error handling and detailed logging.
- **ETL Management**: Solutions to automate extraction, transformation, and loading tasks from various data sources.
- **Monitoring and Alerting**: Scripts to monitor the health of data systems, task status, and send alerts in case of failures.
- **Professional Code**: Well-structured scripts, with clear comments, following shell scripting best practices to ensure readability and maintainability.
- **Complete Documentation**: Each script is accompanied by detailed documentation, explaining its purpose, parameters, logic, and usage examples.
- **Tests Included**: Examples of how to test Bash scripts to ensure their correctness and robustness.

### üìä Visualization

```mermaid
graph TB
    subgraph "Data Pipeline (data_pipeline.sh)"
        A1[Start Pipeline] --> B1[Load Configuration]
        B1 --> C1[Extract Data]
        C1 --> D1[Validate Data]
        D1 --> E1{Valid?}
        E1 -->|Yes| F1[Transform & Enrich]
        E1 -->|No| G1[Move to Error Dir]
        F1 --> H1[Aggregate by Category & Date]
        H1 --> I1[Load to Processed Dir]
        I1 --> J1[Send Success Notification]
        G1 --> K1[Send Error Notification]
    end
    
    subgraph "Log Analyzer (log_analyzer.sh)"
        A2[Start Analysis] --> B2[Generate/Load Logs]
        B2 --> C2[Analyze Log Levels]
        C2 --> D2[Analyze Services]
        D2 --> E2[Analyze Response Times]
        E2 --> F2[Detect Time Patterns]
        F2 --> G2[Detect Anomalies]
        G2 --> H2[Generate Summary Report]
        H2 --> I2[Output to Analysis Dir]
    end
    
    style A1 fill:#90EE90,stroke:#333,stroke-width:2px
    style J1 fill:#90EE90,stroke:#333,stroke-width:2px
    style K1 fill:#FFB6C1,stroke:#333,stroke-width:2px
    style A2 fill:#87CEEB,stroke:#333,stroke-width:2px
    style I2 fill:#87CEEB,stroke:#333,stroke-width:2px
```

*Diagramas ilustrativos do fluxo de automa√ß√£o de opera√ß√µes de dados em Bash, destacando as etapas e intera√ß√µes. Para mais detalhes, veja [ARCHITECTURE.md](diagrams/ARCHITECTURE.md).*


---

## üìö Scripts Dispon√≠veis / Available Scripts

### 1. data_pipeline.sh - Pipeline Completa de Dados

Script de pipeline ETL completo com valida√ß√£o robusta, transforma√ß√µes complexas e tratamento de erros.

**Funcionalidades:**
- ‚úÖ Extra√ß√£o de dados simulada (CSV)
- ‚úÖ Valida√ß√£o robusta com m√∫ltiplos crit√©rios
  - Valida√ß√£o de tipos de dados
  - Valida√ß√£o de formato de data
  - Valida√ß√£o de valores num√©ricos
  - Valida√ß√£o de campos obrigat√≥rios
- ‚úÖ Transforma√ß√µes de dados
  - Enriquecimento com lookup tables
  - C√°lculos de m√©tricas (total_amount)
  - Agrega√ß√µes por categoria e data
- ‚úÖ Carregamento de dados processados
- ‚úÖ Logging detalhado com timestamps
- ‚úÖ Tratamento de erros com notifica√ß√µes
- ‚úÖ Configura√ß√£o externa via arquivo
- ‚úÖ Separa√ß√£o de dados v√°lidos e inv√°lidos

**Uso:**
```bash
# Criar estrutura de diret√≥rios
mkdir -p config data/{raw,staging,processed,lookup,errors} logs

# Criar arquivo de configura√ß√£o
cat > config/pipeline_config.conf <<EOF
SOURCE_DIR=./data/raw
STAGING_DIR=./data/staging
PROCESSED_DIR=./data/processed
LOOKUP_DIR=./data/lookup
ERROR_DIR=./data/errors
LOG_FILE=./logs/data_pipeline.log
NOTIFICATION_ENABLED=false
EOF

# Executar pipeline
bash src/data_pipeline.sh
```

**Sa√≠da:**
- `data/staging/valid_data_*.csv` - Dados validados
- `data/errors/invalid_data_*.csv` - Dados com erros de valida√ß√£o
- `data/staging/transformed_*.csv` - Dados agregados por categoria e data
- `data/processed/final_*.csv` - Dados processados finais
- `logs/data_pipeline.log` - Log completo da execu√ß√£o

### 2. log_analyzer.sh - Analisador Avan√ßado de Logs

Ferramenta profissional de an√°lise de logs com estat√≠sticas avan√ßadas e detec√ß√£o de anomalias.

**Funcionalidades:**
- üìä An√°lise de n√≠veis de log (INFO, WARNING, ERROR, DEBUG)
- üîç An√°lise de servi√ßos/componentes
- ‚è±Ô∏è Estat√≠sticas de tempos de resposta
  - M√©dia, mediana, m√≠nimo, m√°ximo
  - Desvio padr√£o
  - Distribui√ß√£o por faixas de tempo
- üìà Padr√µes temporais (distribui√ß√£o por hora)
- üö® Detec√ß√£o de anomalias baseada em thresholds
- üìù Gera√ß√£o de relat√≥rios completos
- üé® Output colorido para melhor visualiza√ß√£o

**Uso:**
```bash
# Executar com logs de exemplo
bash src/log_analyzer.sh

# Executar com logs personalizados
export LOG_DIR=./meus_logs
export OUTPUT_DIR=./minhas_analises
bash src/log_analyzer.sh
```

**Sa√≠da:**
- `analysis/log_levels_summary.txt` - Resumo de n√≠veis de log
- `analysis/services_summary.txt` - Resumo de servi√ßos
- `analysis/response_times_summary.txt` - Estat√≠sticas de tempos de resposta
- `analysis/time_patterns_summary.txt` - Padr√µes temporais
- `analysis/anomalies_report.txt` - Relat√≥rio de anomalias
- `analysis/summary_report.txt` - Relat√≥rio resumido geral

**Configura√ß√£o:**
```bash
# Vari√°veis de ambiente opcionais
export LOG_DIR="./logs"                    # Diret√≥rio de logs
export OUTPUT_DIR="./analysis"             # Diret√≥rio de sa√≠da
export ALERT_THRESHOLD_ERROR=10            # Threshold de erros
export ALERT_THRESHOLD_WARNING=50          # Threshold de warnings
```

---

## üõ†Ô∏è Tecnologias Utilizadas / Technologies Used

| Categoria         | Tecnologia      | Descri√ß√£o                                                                 |
| :---------------- | :-------------- | :------------------------------------------------------------------------ |
| **Linguagem**     | Bash            | Linguagem de script shell para automa√ß√£o de tarefas.                      |
| **Ferramentas CLI** | `awk`, `sed`, `grep` | Utilit√°rios padr√£o do Unix para manipula√ß√£o e processamento de texto.     |
| **Processamento Num√©rico** | `datamash`      | Ferramenta de linha de comando para opera√ß√µes num√©ricas e estat√≠sticas.   |
| **Agendamento**   | `cron`          | (Conceitual) Para agendamento de execu√ß√£o de scripts em intervalos regulares. |
| **Logging**       | `tee`           | Para direcionar a sa√≠da para o console e para um arquivo de log.          |
| **Testes**        | `shellcheck`    | (Conceitual) Ferramenta est√°tica para an√°lise de scripts Bash.            |
| **Diagrama√ß√£o**   | Mermaid         | Para cria√ß√£o de diagramas de arquitetura e fluxo de dados no README.      |

---

## üìÅ Repository Structure

```
bash-data-ops-automation/
‚îú‚îÄ‚îÄ src/                 # Scripts Bash para automa√ß√£o de tarefas DataOps
‚îÇ   ‚îú‚îÄ‚îÄ data_pipeline.sh # Pipeline completa de dados com ETL
‚îÇ   ‚îî‚îÄ‚îÄ log_analyzer.sh  # Analisador avan√ßado de logs
‚îú‚îÄ‚îÄ tests/               # Scripts de teste para valida√ß√£o dos scripts Bash
‚îÇ   ‚îú‚îÄ‚îÄ run_all_tests.sh        # Runner principal de todos os testes
‚îÇ   ‚îú‚îÄ‚îÄ test_data_pipeline.sh   # Testes do data_pipeline.sh
‚îÇ   ‚îî‚îÄ‚îÄ test_log_analyzer.sh    # Testes do log_analyzer.sh
‚îú‚îÄ‚îÄ diagrams/            # Diagramas de arquitetura e fluxo de dados
‚îÇ   ‚îî‚îÄ‚îÄ bash_data_ops_flow.png  # Diagrama do fluxo DataOps
‚îú‚îÄ‚îÄ images/              # Imagens e screenshots para documenta√ß√£o
‚îú‚îÄ‚îÄ analysis/            # Diret√≥rio de sa√≠da para an√°lises de logs
‚îú‚îÄ‚îÄ logs/                # Diret√≥rio para armazenar logs de execu√ß√£o da pipeline
‚îú‚îÄ‚îÄ .gitignore           # Arquivos e diret√≥rios ignorados pelo git
‚îú‚îÄ‚îÄ LICENSE              # Licen√ßa MIT do projeto
‚îî‚îÄ‚îÄ README.md            # Este arquivo
```

---

## üöÄ Getting Started

Para come√ßar, clone o reposit√≥rio e execute a demonstra√ß√£o completa:

### Quick Start

```bash
# Clonar o reposit√≥rio
git clone https://github.com/galafis/bash-data-ops-automation.git
cd bash-data-ops-automation

# Instalar depend√™ncias (Ubuntu/Debian)
sudo apt-get update && sudo apt-get install -y datamash bc

# Executar demonstra√ß√£o completa
bash demo.sh
```

O script de demonstra√ß√£o executar√°:
1. ‚úÖ Pipeline completa de dados (ETL) com valida√ß√£o e transforma√ß√µes
2. ‚úÖ An√°lise avan√ßada de logs com estat√≠sticas e detec√ß√£o de anomalias  
3. ‚úÖ Suite completa de testes automatizados

### Executar Scripts Individualmente

### Executar Scripts Individualmente

**Pipeline de Dados:**
```bash
# Criar estrutura
mkdir -p config data/{raw,staging,processed,lookup,errors} logs

# Configurar
cat > config/pipeline_config.conf <<EOF
SOURCE_DIR=./data/raw
STAGING_DIR=./data/staging
PROCESSED_DIR=./data/processed
LOOKUP_DIR=./data/lookup
ERROR_DIR=./data/errors
LOG_FILE=./logs/data_pipeline.log
NOTIFICATION_ENABLED=false
EOF

# Executar
bash src/data_pipeline.sh
```

**An√°lise de Logs:**
```bash
# Executar (gera logs de exemplo automaticamente)
bash src/log_analyzer.sh

# Ver resultados
cat analysis/anomalies_report.txt
```

### Pr√©-requisitos

- Ambiente Linux/Unix com Bash.
- Utilit√°rios padr√£o do sistema (`awk`, `sed`, `grep`, `cp`, `mkdir`, `wc`, `date`, `tee`).
- `datamash` (pode ser instalado via gerenciador de pacotes do sistema, e.g., `sudo apt-get install datamash` no Ubuntu).

### Instala√ß√£o

```bash
git clone https://github.com/GabrielDemetriosLafis/bash-data-ops-automation.git
cd bash-data-ops-automation

# Instalar datamash se ainda n√£o estiver instalado
sudo apt-get update && sudo apt-get install -y datamash

# Criar diret√≥rios necess√°rios
mkdir -p data/raw data/staging data/processed logs config

# Criar um arquivo de configura√ß√£o de exemplo
echo "SOURCE_DIR=./data/raw" > config/pipeline_config.conf
echo "STAGING_DIR=./data/staging" >> config/pipeline_config.conf
echo "PROCESSED_DIR=./data/processed" >> config/pipeline_config.conf
echo "LOG_FILE=./logs/data_pipeline.log" >> config/pipeline_config.conf
```

### Exemplo de Uso Avan√ßado (Bash)

O exemplo abaixo demonstra uma pipeline de dados completa em Bash, incluindo extra√ß√£o simulada, valida√ß√£o robusta, transforma√ß√µes complexas (agrega√ß√£o e enriquecimento), carregamento e um sistema de notifica√ß√£o de status. Este script ilustra como orquestrar tarefas DataOps de forma eficiente e resiliente.

```bash
#!/bin/bash

# Bash DataOps Automation - Data Pipeline Example
# Author: Gabriel Demetrios Lafis
# Year: 2025

# Este script demonstra uma pipeline de dados completa usando Bash.
# Ele simula a extra√ß√£o, transforma√ß√£o e carregamento de dados com tratamento de erros e logging.

set -o pipefail # Garante que a pipeline falhe se qualquer comando falhar

# --- Configura√ß√µes ---
CONFIG_FILE="./config/pipeline_config.conf"

# Carregar configura√ß√µes do arquivo
load_config() {
    if [ -f "$CONFIG_FILE" ]; then
        source "$CONFIG_FILE"
        log_message "Configura√ß√µes carregadas de $CONFIG_FILE"
    else
        log_message "ERRO: Arquivo de configura√ß√£o $CONFIG_FILE n√£o encontrado. Abortando."
        exit 1
    fi
}

# Fun√ß√£o para logar mensagens
log_message() {
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOG_FILE"
}

# Fun√ß√£o para tratamento de erros
handle_error() {
    local exit_code=$?
    local step_name="$1"
    log_message "ERRO: A pipeline falhou na etapa '$step_name' com o c√≥digo de sa√≠da $exit_code."
    # Adicionar l√≥gica de notifica√ß√£o aqui (e.g., enviar e-mail)
    # send_email_notification "Pipeline Falhou" "A pipeline de dados falhou na etapa $step_name. Verifique os logs em $LOG_FILE."
    exit $exit_code
}

# Fun√ß√£o para validar dados (exemplo mais robusto)
validate_data() {
    local file="$1"
    log_message "Validando dados em $file..."
    if [ ! -f "$file" ]; then
        log_message "ERRO de Valida√ß√£o: Arquivo $file n√£o encontrado."
        return 1
    fi
    if [ $(wc -l < "$file") -lt 2 ]; then # Pelo menos cabe√ßalho e uma linha de dados
        log_message "ERRO de Valida√ß√£o: Arquivo $file est√° vazio ou cont√©m apenas cabe√ßalho."
        return 1
    fi
    # Exemplo de valida√ß√£o de formato (verificar se todas as linhas t√™m 3 colunas CSV)
    if ! awk -F',' 'NR > 1 && NF != 3 {print "Linha inv√°lida: " $0; exit 1}' "$file"; then
        log_message "ERRO de Valida√ß√£o: Formato de CSV inv√°lido em $file."
        return 1
    fi
    log_message "Valida√ß√£o de dados em $file conclu√≠da com sucesso."
    return 0
}

# --- Etapa 1: Extra√ß√£o (Simulada) ---
extract_data() {
    log_message "Iniciando etapa de Extra√ß√£o..."
    mkdir -p "$SOURCE_DIR" || handle_error "Cria√ß√£o de diret√≥rio SOURCE_DIR"
    # Simula a cria√ß√£o de um arquivo de dados brutos com mais colunas e um poss√≠vel erro
    echo "transaction_id,customer_id,amount,product_category,timestamp" > "$SOURCE_DIR/raw_transactions.csv"
    echo "TRN001,CUST001,100.50,Electronics,$(date +%s)" >> "$SOURCE_DIR/raw_transactions.csv"
    echo "TRN002,CUST002,200.75,Books,$(date +%s)" >> "$SOURCE_DIR/raw_transactions.csv"
    echo "TRN003,CUST003,150.00,Electronics,$(date +%s)" >> "$SOURCE_DIR/raw_transactions.csv"
    echo "TRN004,CUST004,50.00,Clothing,$(date +%s)" >> "$SOURCE_DIR/raw_transactions.csv"
    echo "TRN005,CUST005,250.25,Electronics,$(date +%s)" >> "$SOURCE_DIR/raw_transactions.csv"
    echo "TRN006,CUST006,75.00,Books,$(date +%s)" >> "$SOURCE_DIR/raw_transactions.csv"
    echo "TRN007,CUST007,300.00,Electronics,$(date +%s)" >> "$SOURCE_DIR/raw_transactions.csv"
    echo "TRN008,CUST008,120.00,Clothing,$(date +%s)" >> "$SOURCE_DIR/raw_transactions.csv"
    echo "TRN009,CUST009,20.00,Books,$(date +%s)" >> "$SOURCE_DIR/raw_transactions.csv"
    echo "TRN010,CUST010,180.00,Electronics,$(date +%s)" >> "$SOURCE_DIR/raw_transactions.csv"
    # Adicionar uma linha com erro de formato para teste de valida√ß√£o
    echo "TRN011,CUST011,INVALID_AMOUNT,Books,$(date +%s)" >> "$SOURCE_DIR/raw_transactions.csv"

    log_message "Extra√ß√£o conclu√≠da. Arquivo gerado em $SOURCE_DIR/raw_transactions.csv"
}

# --- Etapa 2: Transforma√ß√£o (Simulada) ---
transform_data() {
    log_message "Iniciando etapa de Transforma√ß√£o..."
    mkdir -p "$STAGING_DIR" || handle_error "Cria√ß√£o de diret√≥rio STAGING_DIR"
    local raw_file="$SOURCE_DIR/raw_transactions.csv"
    local transformed_file="$STAGING_DIR/transformed_transactions.csv"
    local aggregated_file="$STAGING_DIR/aggregated_sales_by_category.csv"

    validate_data "$raw_file" || handle_error "Valida√ß√£o de dados brutos"

    # Transforma√ß√£o 1: Filtrar registros com valores num√©ricos inv√°lidos e adicionar coluna de imposto
    log_message "Aplicando transforma√ß√£o: filtrando e adicionando imposto..."
    awk -F',' 'BEGIN {OFS=","} NR==1 {print $0, "tax_amount"} NR>1 {if ($3 ~ /^[0-9]+\.?[0-9]*$/) {print $0, $3 * 0.05} else {print $0, "0.00"}}' "$raw_file" | \
    grep -v ",INVALID_AMOUNT," > "$transformed_file"
    
    # Transforma√ß√£o 2: Agrega√ß√£o de vendas por categoria usando datamash
    log_message "Aplicando transforma√ß√£o: agregando vendas por categoria..."
    echo "product_category,total_sales_amount,average_sale_amount,transaction_count" > "$aggregated_file"
    tail -n +2 "$transformed_file" | \
    cut -d',' -f4,3 | \
    datamash -t',' groupby 1 sum 2 mean 2 count 2 >> "$aggregated_file"

    log_message "Transforma√ß√£o conclu√≠da. Arquivos gerados em $transformed_file e $aggregated_file"
}

# --- Etapa 3: Carregamento (Simulada) ---
load_data() {
    log_message "Iniciando etapa de Carregamento..."
    mkdir -p "$PROCESSED_DIR" || handle_error "Cria√ß√£o de diret√≥rio PROCESSED_DIR"
    local transformed_file="$STAGING_DIR/transformed_transactions.csv"
    local aggregated_file="$STAGING_DIR/aggregated_sales_by_category.csv"

    validate_data "$transformed_file" || handle_error "Valida√ß√£o de dados transformados (transa√ß√µes)"
    validate_data "$aggregated_file" || handle_error "Valida√ß√£o de dados transformados (agregados)"

    # Simula o carregamento: copia os arquivos transformados para o diret√≥rio final
    cp "$transformed_file" "$PROCESSED_DIR/final_transactions.csv" || handle_error "C√≥pia de final_transactions.csv"
    cp "$aggregated_file" "$PROCESSED_DIR/final_aggregated_sales.csv" || handle_error "C√≥pia de final_aggregated_sales.csv"
    
    log_message "Carregamento conclu√≠do. Arquivos finais em $PROCESSED_DIR/final_transactions.csv e $PROCESSED_DIR/final_aggregated_sales.csv"
}

# --- Execu√ß√£o da Pipeline ---
run_pipeline() {
    log_message "========================================="
    log_message "Iniciando pipeline de dados..."
    log_message "========================================="

    load_config
    mkdir -p "./logs" "$SOURCE_DIR" "$STAGING_DIR" "$PROCESSED_DIR" || handle_error "Cria√ß√£o de diret√≥rios"

    # Limpar arquivos de sa√≠da anteriores para garantir um teste limpo
    rm -f "$SOURCE_DIR"/* "$STAGING_DIR"/* "$PROCESSED_DIR"/* "$LOG_FILE"

    trap 'handle_error "Extra√ß√£o"' ERR
    extract_data

    trap 'handle_error "Transforma√ß√£o"' ERR
    transform_data

    trap 'handle_error "Carregamento"' ERR
    load_data

    log_message "========================================="
    log_message "Pipeline de dados conclu√≠da com sucesso!"
    log_message "========================================="
    # Adicionar l√≥gica de notifica√ß√£o de sucesso aqui
    # send_email_notification "Pipeline Conclu√≠da" "A pipeline de dados foi conclu√≠da com sucesso. Verifique os resultados em $PROCESSED_DIR."
}

# Executar a pipeline
run_pipeline
```

---

## üß™ Testes / Testing

Este projeto inclui uma su√≠te de testes abrangente para garantir a qualidade e confiabilidade dos scripts.

### Executando os Testes

```bash
# Executar todos os testes
cd tests
bash run_all_tests.sh

# Executar teste espec√≠fico do pipeline de dados
bash tests/test_data_pipeline.sh

# Executar teste espec√≠fico do analisador de logs
bash tests/test_log_analyzer.sh
```

### Estrutura dos Testes

- **test_data_pipeline.sh**: Testa todas as funcionalidades do pipeline de dados
  - Extra√ß√£o de dados simulada
  - Valida√ß√£o robusta com detec√ß√£o de erros
  - Transforma√ß√µes e agrega√ß√µes complexas
  - Carregamento de dados processados
  - Verifica√ß√£o de arquivos de sa√≠da

- **test_log_analyzer.sh**: Testa o analisador de logs
  - Gera√ß√£o de logs de exemplo
  - An√°lise de n√≠veis de log
  - An√°lise de servi√ßos
  - Estat√≠sticas de tempos de resposta
  - Detec√ß√£o de padr√µes temporais
  - Relat√≥rios de anomalias


### Cobertura de Testes

‚úÖ Todos os scripts principais possuem testes automatizados  
‚úÖ Valida√ß√£o de entrada e sa√≠da  
‚úÖ Tratamento de erros  
‚úÖ Casos de sucesso e falha  
‚úÖ Integra√ß√£o de testes automatizados  

---

## üìä Exemplos de Uso / Usage Examples

### Pipeline de Dados

Execute o pipeline completo de dados:

```bash
cd bash-data-ops-automation
mkdir -p config data/{raw,staging,processed,lookup,errors} logs

# Criar arquivo de configura√ß√£o
cat > config/pipeline_config.conf <<EOF
SOURCE_DIR=./data/raw
STAGING_DIR=./data/staging
PROCESSED_DIR=./data/processed
LOOKUP_DIR=./data/lookup
ERROR_DIR=./data/errors
LOG_FILE=./logs/data_pipeline.log
NOTIFICATION_ENABLED=false
EOF

# Executar pipeline
bash src/data_pipeline.sh
```

### An√°lise de Logs

Execute a an√°lise avan√ßada de logs:

```bash
cd bash-data-ops-automation

# Executar an√°lise (gera logs de exemplo automaticamente)
bash src/log_analyzer.sh

# Verificar resultados
ls -la analysis/
cat analysis/log_levels_summary.txt
cat analysis/anomalies_report.txt
```

### Sa√≠da Esperada

**Pipeline de Dados:**
```
[2025-10-15 23:00:00] [INFO] =========================================
[2025-10-15 23:00:00] [INFO] Iniciando pipeline de dados principal...
[2025-10-15 23:00:00] [INFO] =========================================
[2025-10-15 23:00:00] [INFO] Iniciando etapa de Extra√ß√£o...
[2025-10-15 23:00:00] [INFO] Extra√ß√£o conclu√≠da...
[2025-10-15 23:00:00] [INFO] Iniciando etapa de Valida√ß√£o de Dados...
[2025-10-15 23:00:00] [INFO] Valida√ß√£o conclu√≠da...
[2025-10-15 23:00:00] [INFO] Iniciando etapa de Transforma√ß√£o...
[2025-10-15 23:00:00] [INFO] Transforma√ß√£o conclu√≠da...
[2025-10-15 23:00:00] [INFO] Iniciando etapa de Carregamento...
[2025-10-15 23:00:00] [INFO] Pipeline de dados conclu√≠da com sucesso!
```

**An√°lise de Logs:**
```
=========================================
Log Analyzer - Advanced Log Analysis
=========================================
[INFO] Gerando 1000 linhas de logs de exemplo...
[SUCCESS] Logs de exemplo gerados...
[INFO] Analisando n√≠veis de log...
[SUCCESS] An√°lise de n√≠veis salva...
[SUCCESS] An√°lise completa! Todos os relat√≥rios foram salvos em ./analysis/
```

---

## ü§ù Contribui√ß√£o

Contribui√ß√µes s√£o bem-vindas! Sinta-se √† vontade para abrir issues, enviar pull requests ou sugerir melhorias.

Para contribuir:
1. Fork o reposit√≥rio
2. Crie uma branch para sua feature (`git checkout -b feature/MinhaFeature`)
3. Commit suas mudan√ßas (`git commit -m 'feat: adiciona MinhaFeature'`)
4. Push para a branch (`git push origin feature/MinhaFeature`)
5. Abra um Pull Request

Por favor, leia o [CONTRIBUTING.md](CONTRIBUTING.md) para mais detalhes sobre nosso c√≥digo de conduta e o processo de submiss√£o de pull requests.

---

## üìù Licen√ßa

Este projeto est√° licenciado sob a Licen√ßa MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.

---

**Autor:** Gabriel Demetrios Lafis  \n**Ano:** 2025
