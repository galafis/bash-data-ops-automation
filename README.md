# DataOps Automation with Bash

![Bash](https://img.shields.io/badge/Bash-4EAA25?style=for-the-badge&logo=gnu-bash&logoColor=white)
![Datamash](https://img.shields.io/badge/Tool-Datamash-blue?style=for-the-badge&logo=gnu&logoColor=white)
![Mermaid](https://img.shields.io/badge/Diagrams-Mermaid-orange?style=for-the-badge&logo=mermaid&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-green.svg?style=for-the-badge)

---

## üáßüá∑ Automa√ß√£o DataOps com Bash

Este reposit√≥rio √© dedicado a demonstrar **solu√ß√µes pr√°ticas e eficientes para automa√ß√£o de tarefas DataOps utilizando scripts Bash**. O foco √© em como orquestrar, monitorar e gerenciar pipelines de dados, tarefas de ETL (Extract, Transform, Load), e rotinas de manuten√ß√£o de sistemas de dados, aproveitando a robustez e a flexibilidade do ambiente de linha de comando Linux. √â ideal para **engenheiros de dados, administradores de sistemas e profissionais de DevOps** que buscam otimizar e automatizar seus fluxos de trabalho de dados.

### üéØ Objetivo

O principal objetivo deste projeto √© **fornecer um conjunto de scripts Bash bem documentados e exemplos de casos de uso** para automatizar opera√ß√µes comuns em DataOps. Ser√£o abordados t√≥picos como agendamento de tarefas (cron), manipula√ß√£o de arquivos e diret√≥rios, intera√ß√£o com bancos de dados (via CLI), monitoramento de processos e tratamento de erros, tudo com foco em **escalabilidade, confiabilidade e facilidade de manuten√ß√£o**, com √™nfase em **valida√ß√£o de dados robusta, transforma√ß√µes complexas e um sistema de notifica√ß√£o de status**.

### ‚ú® Destaques

- **Valida√ß√£o de Dados Robusta**: Implementa√ß√£o de fun√ß√µes de valida√ß√£o que verificam a integridade e o formato dos dados em diferentes est√°gios da pipeline, garantindo a qualidade dos dados antes do processamento.
- **Transforma√ß√µes de Dados Complexas**: Exemplos de como realizar transforma√ß√µes de dados sofisticadas usando ferramentas de linha de comando como `awk`, `sed`, `grep` e `datamash`, permitindo agrega√ß√£o, enriquecimento e reestrutura√ß√£o de dados.
- **Sistema de Notifica√ß√£o de Status**: Integra√ß√£o de mecanismos de notifica√ß√£o (e.g., e-mail, logs detalhados) para alertar sobre o sucesso, falha ou anomalias na execu√ß√£o da pipeline, facilitando o monitoramento proativo.
- **Automa√ß√£o de Pipelines**: Scripts para orquestrar o fluxo de dados, desde a ingest√£o at√© o processamento e carregamento, com tratamento de erros e logging detalhado.
- **Gerenciamento de ETL**: Solu√ß√µes para automatizar tarefas de extra√ß√£o, transforma√ß√£o e carregamento de dados de diversas fontes.
- **Monitoramento e Alerta**: Scripts para monitorar a sa√∫de dos sistemas de dados, o status das tarefas e enviar alertas em caso de falhas.
- **C√≥digo Profissional**: Scripts bem estruturados, com coment√°rios claros, seguindo as melhores pr√°ticas de shell scripting para garantir legibilidade e manutenibilidade.
- **Documenta√ß√£o Completa**: Cada script √© acompanhado de documenta√ß√£o detalhada, explicando seu prop√≥sito, par√¢metros, l√≥gica e exemplos de uso.
- **Testes Inclu√≠dos**: Exemplos de como testar scripts Bash para garantir sua corre√ß√£o e robustez.

### üöÄ Benef√≠cios do DataOps com Bash em A√ß√£o

A automa√ß√£o DataOps com Bash oferece uma abordagem leve e poderosa para gerenciar pipelines de dados. Este projeto ilustra como esses benef√≠cios s√£o explorados:

1.  **Efici√™ncia e Velocidade**: Scripts Bash s√£o executados diretamente no sistema operacional, proporcionando alta performance para tarefas de manipula√ß√£o de arquivos e processamento de texto, como visto nas transforma√ß√µes com `awk` e `datamash`.

2.  **Flexibilidade e Controle**: A linha de comando oferece controle granular sobre cada etapa da pipeline, permitindo personaliza√ß√£o e adapta√ß√£o a requisitos espec√≠ficos, desde a valida√ß√£o de dados at√© a notifica√ß√£o de status.

3.  **Baixo Custo e Baixa Complexidade**: Utiliza ferramentas nativas do sistema operacional, eliminando a necessidade de infraestrutura complexa ou licen√ßas de software caras, tornando-o acess√≠vel para diversos ambientes.

4.  **Integra√ß√£o com Ferramentas Existentes**: Facilmente integr√°vel com outras ferramentas e sistemas via CLI, permitindo a constru√ß√£o de pipelines h√≠bridas e a intera√ß√£o com bancos de dados, APIs e servi√ßos de nuvem.

5.  **Robustez e Tratamento de Erros**: A inclus√£o de `set -o pipefail` e fun√ß√µes de `handle_error` demonstra como construir pipelines resilientes que podem lidar com falhas de forma controlada e notificar os respons√°veis.

6.  **Simplicidade e Manutenibilidade**: Scripts bem escritos e documentados s√£o f√°ceis de entender, depurar e manter, mesmo para equipes com conhecimento limitado em linguagens de programa√ß√£o mais complexas.

---

## üá¨üáß DataOps Automation with Bash

This repository is dedicated to demonstrating **practical and efficient solutions for DataOps task automation using Bash scripts**. The focus is on how to orchestrate, monitor, and manage data pipelines, ETL (Extract, Transform, Load) tasks, and data system maintenance routines, leveraging the robustness and flexibility of the Linux command-line environment. It is ideal for **data engineers, system administrators, and DevOps professionals** looking to optimize and automate their data workflows.

### üéØ Objective

The main objective of this project is to **provide a set of well-documented Bash scripts and use case examples** to automate common operations in DataOps. Topics covered include task scheduling (cron), file and directory manipulation, database interaction (via CLI), process monitoring, and error handling, all with a focus on **scalability, reliability, and ease of maintenance**, with an emphasis on **robust data validation, complex transformations, and a status notification system**.

### ‚ú® Highlights

- **Robust Data Validation**: Implementation of validation functions that check data integrity and format at different pipeline stages, ensuring data quality before processing.
- **Complex Data Transformations**: Examples of how to perform sophisticated data transformations using command-line tools like `awk`, `sed`, `grep`, and `datamash`, allowing for data aggregation, enrichment, and restructuring.
- **Status Notification System**: Integration of notification mechanisms (e.g., email, detailed logs) to alert about pipeline success, failure, or anomalies, facilitating proactive monitoring.
- **Pipeline Automation**: Scripts for orchestrating data flow, from ingestion to processing and loading, with error handling and detailed logging.
- **ETL Management**: Solutions to automate extraction, transformation, and loading tasks from various data sources.
- **Monitoring and Alerting**: Scripts to monitor the health of data systems, task status, and send alerts in case of failures.
- **Professional Code**: Well-structured scripts, with clear comments, following shell scripting best practices to ensure readability and maintainability.
- **Complete Documentation**: Each script is accompanied by detailed documentation, explaining its purpose, parameters, logic, and usage examples.
- **Tests Included**: Examples of how to test Bash scripts to ensure their correctness and robustness.

### üìä Visualization

![Bash DataOps Flow](diagrams/bash_data_ops_flow.png)

*Diagrama ilustrativo do fluxo de automa√ß√£o de opera√ß√µes de dados em Bash, destacando as etapas e intera√ß√µes.*


---

## üõ†Ô∏è Tecnologias Utilizadas / Technologies Used

| Categoria         | Tecnologia      | Descri√ß√£o                                                                 |
| :---------------- | :-------------- | :------------------------------------------------------------------------ |
| **Linguagem**     | Bash            | Linguagem de script shell para automa√ß√£o de tarefas.                      |
| **Ferramentas CLI** | `awk`, `sed`, `grep` | Utilit√°rios padr√£o do Unix para manipula√ß√£o e processamento de texto.     |
| **Processamento Num√©rico** | `datamash`      | Ferramenta de linha de comando para opera√ß√µes num√©ricas e estat√≠sticas.   |
| **Agendamento**   | `cron`          | (Conceitual) Para agendamento de execu√ß√£o de scripts em intervalos regulares. |
| **Logging**       | `tee`           | Para direcionar a sa√≠da para o console e para um arquivo de log.          |
| **Testes**        | `shellcheck`    | (Conceitual) Ferramenta est√°tica para an√°lise de scripts Bash.            |
| **Diagrama√ß√£o**   | Mermaid         | Para cria√ß√£o de diagramas de arquitetura e fluxo de dados no README.      |

---

## üìÅ Repository Structure

```
bash-data-ops-automation/
‚îú‚îÄ‚îÄ src/           # Scripts Bash para automa√ß√£o de tarefas DataOps
‚îú‚îÄ‚îÄ data/          # Dados de exemplo (CSV) para simular ingest√£o e processamento
‚îú‚îÄ‚îÄ images/        # Imagens e diagramas para o README e documenta√ß√£o
‚îú‚îÄ‚îÄ tests/         # Scripts de teste para valida√ß√£o dos scripts Bash
‚îú‚îÄ‚îÄ docs/          # Documenta√ß√£o adicional, tutoriais e guias de boas pr√°ticas
‚îú‚îÄ‚îÄ config/        # Arquivos de configura√ß√£o para as pipelines
‚îú‚îÄ‚îÄ logs/          # Diret√≥rio para armazenar logs de execu√ß√£o da pipeline
‚îî‚îÄ‚îÄ README.md      # Este arquivo
```

---

## üöÄ Getting Started

Para come√ßar, clone o reposit√≥rio e explore os diret√≥rios `src/` e `docs/` para exemplos detalhados e instru√ß√µes de uso. Certifique-se de ter um ambiente Linux/Unix com Bash e as ferramentas CLI necess√°rias instaladas.

### Pr√©-requisitos

- Ambiente Linux/Unix com Bash.
- Utilit√°rios padr√£o do sistema (`awk`, `sed`, `grep`, `cp`, `mkdir`, `wc`, `date`, `tee`).
- `datamash` (pode ser instalado via gerenciador de pacotes do sistema, e.g., `sudo apt-get install datamash` no Ubuntu).

### Instala√ß√£o

```bash
git clone https://github.com/GabrielDemetriosLafis/bash-data-ops-automation.git
cd bash-data-ops-automation

# Instalar datamash se ainda n√£o estiver instalado
sudo apt-get update && sudo apt-get install -y datamash

# Criar diret√≥rios necess√°rios
mkdir -p data/raw data/staging data/processed logs config

# Criar um arquivo de configura√ß√£o de exemplo
echo "SOURCE_DIR=./data/raw" > config/pipeline_config.conf
echo "STAGING_DIR=./data/staging" >> config/pipeline_config.conf
echo "PROCESSED_DIR=./data/processed" >> config/pipeline_config.conf
echo "LOG_FILE=./logs/data_pipeline.log" >> config/pipeline_config.conf
```

### Exemplo de Uso Avan√ßado (Bash)

O exemplo abaixo demonstra uma pipeline de dados completa em Bash, incluindo extra√ß√£o simulada, valida√ß√£o robusta, transforma√ß√µes complexas (agrega√ß√£o e enriquecimento), carregamento e um sistema de notifica√ß√£o de status. Este script ilustra como orquestrar tarefas DataOps de forma eficiente e resiliente.

```bash
#!/bin/bash

# Bash DataOps Automation - Data Pipeline Example
# Author: Gabriel Demetrios Lafis
# Year: 2025

# Este script demonstra uma pipeline de dados completa usando Bash.
# Ele simula a extra√ß√£o, transforma√ß√£o e carregamento de dados com tratamento de erros e logging.

set -o pipefail # Garante que a pipeline falhe se qualquer comando falhar

# --- Configura√ß√µes ---
CONFIG_FILE="./config/pipeline_config.conf"

# Carregar configura√ß√µes do arquivo
load_config() {
    if [ -f "$CONFIG_FILE" ]; then
        source "$CONFIG_FILE"
        log_message "Configura√ß√µes carregadas de $CONFIG_FILE"
    else
        log_message "ERRO: Arquivo de configura√ß√£o $CONFIG_FILE n√£o encontrado. Abortando."
        exit 1
    fi
}

# Fun√ß√£o para logar mensagens
log_message() {
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOG_FILE"
}

# Fun√ß√£o para tratamento de erros
handle_error() {
    local exit_code=$?
    local step_name="$1"
    log_message "ERRO: A pipeline falhou na etapa '$step_name' com o c√≥digo de sa√≠da $exit_code."
    # Adicionar l√≥gica de notifica√ß√£o aqui (e.g., enviar e-mail)
    # send_email_notification "Pipeline Falhou" "A pipeline de dados falhou na etapa $step_name. Verifique os logs em $LOG_FILE."
    exit $exit_code
}

# Fun√ß√£o para validar dados (exemplo mais robusto)
validate_data() {
    local file="$1"
    log_message "Validando dados em $file..."
    if [ ! -f "$file" ]; then
        log_message "ERRO de Valida√ß√£o: Arquivo $file n√£o encontrado."
        return 1
    fi
    if [ $(wc -l < "$file") -lt 2 ]; then # Pelo menos cabe√ßalho e uma linha de dados
        log_message "ERRO de Valida√ß√£o: Arquivo $file est√° vazio ou cont√©m apenas cabe√ßalho."
        return 1
    fi
    # Exemplo de valida√ß√£o de formato (verificar se todas as linhas t√™m 3 colunas CSV)
    if ! awk -F',' 'NR > 1 && NF != 3 {print "Linha inv√°lida: " $0; exit 1}' "$file"; then
        log_message "ERRO de Valida√ß√£o: Formato de CSV inv√°lido em $file."
        return 1
    fi
    log_message "Valida√ß√£o de dados em $file conclu√≠da com sucesso."
    return 0
}

# --- Etapa 1: Extra√ß√£o (Simulada) ---
extract_data() {
    log_message "Iniciando etapa de Extra√ß√£o..."
    mkdir -p "$SOURCE_DIR" || handle_error "Cria√ß√£o de diret√≥rio SOURCE_DIR"
    # Simula a cria√ß√£o de um arquivo de dados brutos com mais colunas e um poss√≠vel erro
    echo "transaction_id,customer_id,amount,product_category,timestamp" > "$SOURCE_DIR/raw_transactions.csv"
    echo "TRN001,CUST001,100.50,Electronics,$(date +%s)" >> "$SOURCE_DIR/raw_transactions.csv"
    echo "TRN002,CUST002,200.75,Books,$(date +%s)" >> "$SOURCE_DIR/raw_transactions.csv"
    echo "TRN003,CUST003,150.00,Electronics,$(date +%s)" >> "$SOURCE_DIR/raw_transactions.csv"
    echo "TRN004,CUST004,50.00,Clothing,$(date +%s)" >> "$SOURCE_DIR/raw_transactions.csv"
    echo "TRN005,CUST005,250.25,Electronics,$(date +%s)" >> "$SOURCE_DIR/raw_transactions.csv"
    echo "TRN006,CUST006,75.00,Books,$(date +%s)" >> "$SOURCE_DIR/raw_transactions.csv"
    echo "TRN007,CUST007,300.00,Electronics,$(date +%s)" >> "$SOURCE_DIR/raw_transactions.csv"
    echo "TRN008,CUST008,120.00,Clothing,$(date +%s)" >> "$SOURCE_DIR/raw_transactions.csv"
    echo "TRN009,CUST009,20.00,Books,$(date +%s)" >> "$SOURCE_DIR/raw_transactions.csv"
    echo "TRN010,CUST010,180.00,Electronics,$(date +%s)" >> "$SOURCE_DIR/raw_transactions.csv"
    # Adicionar uma linha com erro de formato para teste de valida√ß√£o
    echo "TRN011,CUST011,INVALID_AMOUNT,Books,$(date +%s)" >> "$SOURCE_DIR/raw_transactions.csv"

    log_message "Extra√ß√£o conclu√≠da. Arquivo gerado em $SOURCE_DIR/raw_transactions.csv"
}

# --- Etapa 2: Transforma√ß√£o (Simulada) ---
transform_data() {
    log_message "Iniciando etapa de Transforma√ß√£o..."
    mkdir -p "$STAGING_DIR" || handle_error "Cria√ß√£o de diret√≥rio STAGING_DIR"
    local raw_file="$SOURCE_DIR/raw_transactions.csv"
    local transformed_file="$STAGING_DIR/transformed_transactions.csv"
    local aggregated_file="$STAGING_DIR/aggregated_sales_by_category.csv"

    validate_data "$raw_file" || handle_error "Valida√ß√£o de dados brutos"

    # Transforma√ß√£o 1: Filtrar registros com valores num√©ricos inv√°lidos e adicionar coluna de imposto
    log_message "Aplicando transforma√ß√£o: filtrando e adicionando imposto..."
    awk -F',' 'BEGIN {OFS=","} NR==1 {print $0, "tax_amount"} NR>1 {if ($3 ~ /^[0-9]+\.?[0-9]*$/) {print $0, $3 * 0.05} else {print $0, "0.00"}}' "$raw_file" | \
    grep -v ",INVALID_AMOUNT," > "$transformed_file"
    
    # Transforma√ß√£o 2: Agrega√ß√£o de vendas por categoria usando datamash
    log_message "Aplicando transforma√ß√£o: agregando vendas por categoria..."
    echo "product_category,total_sales_amount,average_sale_amount,transaction_count" > "$aggregated_file"
    tail -n +2 "$transformed_file" | \
    cut -d',' -f4,3 | \
    datamash -t',' groupby 1 sum 2 mean 2 count 2 >> "$aggregated_file"

    log_message "Transforma√ß√£o conclu√≠da. Arquivos gerados em $transformed_file e $aggregated_file"
}

# --- Etapa 3: Carregamento (Simulada) ---
load_data() {
    log_message "Iniciando etapa de Carregamento..."
    mkdir -p "$PROCESSED_DIR" || handle_error "Cria√ß√£o de diret√≥rio PROCESSED_DIR"
    local transformed_file="$STAGING_DIR/transformed_transactions.csv"
    local aggregated_file="$STAGING_DIR/aggregated_sales_by_category.csv"

    validate_data "$transformed_file" || handle_error "Valida√ß√£o de dados transformados (transa√ß√µes)"
    validate_data "$aggregated_file" || handle_error "Valida√ß√£o de dados transformados (agregados)"

    # Simula o carregamento: copia os arquivos transformados para o diret√≥rio final
    cp "$transformed_file" "$PROCESSED_DIR/final_transactions.csv" || handle_error "C√≥pia de final_transactions.csv"
    cp "$aggregated_file" "$PROCESSED_DIR/final_aggregated_sales.csv" || handle_error "C√≥pia de final_aggregated_sales.csv"
    
    log_message "Carregamento conclu√≠do. Arquivos finais em $PROCESSED_DIR/final_transactions.csv e $PROCESSED_DIR/final_aggregated_sales.csv"
}

# --- Execu√ß√£o da Pipeline ---
run_pipeline() {
    log_message "========================================="
    log_message "Iniciando pipeline de dados..."
    log_message "========================================="

    load_config
    mkdir -p "./logs" "$SOURCE_DIR" "$STAGING_DIR" "$PROCESSED_DIR" || handle_error "Cria√ß√£o de diret√≥rios"

    # Limpar arquivos de sa√≠da anteriores para garantir um teste limpo
    rm -f "$SOURCE_DIR"/* "$STAGING_DIR"/* "$PROCESSED_DIR"/* "$LOG_FILE"

    trap 'handle_error "Extra√ß√£o"' ERR
    extract_data

    trap 'handle_error "Transforma√ß√£o"' ERR
    transform_data

    trap 'handle_error "Carregamento"' ERR
    load_data

    log_message "========================================="
    log_message "Pipeline de dados conclu√≠da com sucesso!"
    log_message "========================================="
    # Adicionar l√≥gica de notifica√ß√£o de sucesso aqui
    # send_email_notification "Pipeline Conclu√≠da" "A pipeline de dados foi conclu√≠da com sucesso. Verifique os resultados em $PROCESSED_DIR."
}

# Executar a pipeline
run_pipeline
```

---

## ü§ù Contribui√ß√£o

Contribui√ß√µes s√£o bem-vindas! Sinta-se √† vontade para abrir issues, enviar pull requests ou sugerir melhorias. Por favor, siga as diretrizes de contribui√ß√£o.

---

## üìù Licen√ßa

Este projeto est√° licenciado sob a Licen√ßa MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.

